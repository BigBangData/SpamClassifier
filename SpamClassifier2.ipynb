{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying Mr. Geron's Spam Classifier Notebook  - Part II\n",
    "\n",
    "Code often borrowed from [Aur√©lien Geron's famous Jupyter Notebook on Classification.](https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb)\n",
    "\n",
    "Data can be pulled from [Apache SpamAssassin's old corpus.](http://spamassassin.apache.org/old/publiccorpus/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-07-19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import custom_functions as F # see custom module for code\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "dt_object = str(dt_object).split('.')[0]\n",
    "Date, StartTime = dt_object.split(' ')\n",
    "print('Revised on: ' + Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "Study structure of sparse matrix and how to save it instead of pre-processing the data every time.\n",
    "\n",
    "- *Why saving it with scipy.sparse or pickle did not work as expected?*\n",
    "\n",
    "The **WordCounterToVectorTransformer()**'s `vocabulary_` attribute is not saved - this is because we do not return it in the class. \n",
    "\n",
    "The `vocabulary_` remains in the **WordCounterToVectorTransformer()**' class in the Jupyter Notebook envinroment after it trains `X_train`, but it is not saved in the output of the pipeline. This way when we run the `X_test` pipeline we get no errors. By simply importing the saved sparse matrix we get the missing vocabulary error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "Random sampling 10% of the data for quick troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully downloaded.\n"
     ]
    }
   ],
   "source": [
    "F.get_data_if_needed('spam', 'easy_ham', '20030228')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250.0, 50.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting emails\n",
    "data_dir = 'data'\n",
    "spam_dir = os.path.join(data_dir, 'spam')\n",
    "ham_dir = os.path.join(data_dir, 'easy_ham')\n",
    "\n",
    "ham_filenames = [name for name in sorted(os.listdir(ham_dir)) if name != 'cmds']\n",
    "spam_filenames = [name for name in sorted(os.listdir(spam_dir)) if name != 'cmds']\n",
    "\n",
    "len(ham_filenames)/10, len(spam_filenames)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "            \n",
    "ham_sample = random.sample(ham_filenames, 250)\n",
    "spam_sample = random.sample(spam_filenames, 50)\n",
    "\n",
    "spam = F.extract_emails(_path=spam_dir, _names=spam_sample)\n",
    "ham = F.extract_emails(_path=ham_dir, _names=ham_sample)\n",
    "\n",
    "len(ham), len(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test datasets\n",
    "\n",
    "We need to split the traing and test sets before gaining too much information on the test set and biasing ourselves in creating the features for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham + spam)\n",
    "y = np.array([0] * len(ham) + [1] * len(spam))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess, Train, Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer_plusvocab(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "      \n",
    "        # CHANGE - needs vocabulary returned to save and run with test set\n",
    "        return (self.vocabulary_, \n",
    "                csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stopwords included\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", F.EmailToWordCounterTransformer_revised(remove_stopwords=False)),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer_plusvocab()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data if necessary\n",
    "vocabulary_, X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of X counts of words in a specific email, given that a vocabulary_ exists\n",
    "def get_words_counts(X_transformed, X_counts):\n",
    "    list_of_counts = X_transformed.toarray()[0][1:X_counts].tolist()\n",
    "    word_list = [(word, index) for (word, index) in vocabulary_.items() if index < (X_counts+1)] # needs a vocabulary_\n",
    "    out = [(count, word) for (word, index), count in zip(word_list, list_of_counts)]\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 'number'),\n",
       " (7, 'the'),\n",
       " (6, 'to'),\n",
       " (3, 'a'),\n",
       " (3, 'and'),\n",
       " (1, 'of'),\n",
       " (2, 'in'),\n",
       " (7, 'i'),\n",
       " (9, 'it'),\n",
       " (3, 'is'),\n",
       " (1, 'url'),\n",
       " (1, 'for'),\n",
       " (4, 'that'),\n",
       " (4, 'you'),\n",
       " (0, 's'),\n",
       " (2, 'thi'),\n",
       " (0, 'on'),\n",
       " (2, 'with'),\n",
       " (3, 'have')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_counts(X_train_transformed[1], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Date:        Mon, 26 Aug 2002 09:27:56 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030804078.e8b0d5@DeepEddy.Com>\n",
      "    Message-ID:  <1030372078.11075.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "  | Tell me what keystroke made it happen so I can reproduce it and I'll\n",
      "  | see what I can do about it (or if I can't, I'll hand it off to Brent).\n",
      "\n",
      "Don't worry too much about it, you seem to have plenty of other things\n",
      "to do in the immediate future, and this one isn't so critical that people\n",
      "can't use the code in normal ways.\n",
      "\n",
      "But, to make it happen, type (with normal key bindings) any digit, so the\n",
      "code thinks you're trying a message number, then backspace, so the digit\n",
      "goes away, then '-' (other junk characters don't seem to have the\n",
      "problem, I have just been playing).   That will do it (every time).\n",
      "\n",
      "That is: 0 ^h -\n",
      "\n",
      "Once you get into that state, the same traceback occurs for every\n",
      "character you type, until a message is selected with the mouse.\n",
      "\n",
      "This is looking like it might be easy to find and fix, so I'll take a\n",
      "look at it later.\n",
      "\n",
      "kre\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check counts\n",
    "print(X_train[1].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIX\n",
    "\n",
    "How to fix the vocabulary problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forget processed data\n",
    "vocabulary_ = []\n",
    "X_train_transformed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_X_train(vocab_name, X_train_name, X_train):\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import scipy.sparse\n",
    "    \n",
    "    # setup directory and file paths\n",
    "    path = 'processed_data'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)       \n",
    "    vocab_path = os.path.join(path, ''.join([vocab_name, '.json']))\n",
    "    matrix_path = os.path.join(path, ''.join([X_train_name, '.npz']))\n",
    "    \n",
    "    # load vocabulary and matrix if exist\n",
    "    try:\n",
    "        with open(vocab_path, 'r') as fp:\n",
    "            vocabulary_ = json.load(fp)\n",
    "        print('Loading vocabulary.\\n')\n",
    "    except FileNotFoundError as e:  \n",
    "        print('Json file not found.\\n')\n",
    "        pass\n",
    "    try:\n",
    "        X_train_transformed = scipy.sparse.load_npz(matrix_path)\n",
    "        print('Loading sparse matrix.\\n')\n",
    "    except FileNotFoundError as e:  \n",
    "        print('Sparse matrix not found.\\n')\n",
    "        pass\n",
    "    \n",
    "    if 'vocabulary_' in locals() and 'X_train_transformed' in locals():\n",
    "        print('Processed data loaded.')\n",
    "        return(vocabulary_, X_train_transformed)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # if not, process data\n",
    "    try:\n",
    "        vocabulary_, X_train_transformed = preprocess_pipeline.fit_transform(X_train)\n",
    "        print('Processing data...\\n')    \n",
    "    except:\n",
    "        print('Processing error.\\n')\n",
    "        pass\n",
    "    \n",
    "    # save processed data\n",
    "    try:\n",
    "        with open(vocab_path, 'w') as fp:\n",
    "            json.dump(vocabulary_, fp, indent=4)\n",
    "        print('Saving vocabulary...\\n')\n",
    "    except:\n",
    "        print('Error saving vocabulary_...\\n')\n",
    "        pass\n",
    "    try:\n",
    "        scipy.sparse.save_npz(matrix_path, X_train_transformed)\n",
    "        print('Saving sparse matrix...\\n')\n",
    "    except:\n",
    "        print('Error saving matrix...\\n')\n",
    "    \n",
    "    print('Processed data loaded and saved.')   \n",
    "    return(vocabulary_, X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary.\n",
      "\n",
      "Loading sparse matrix.\n",
      "\n",
      "Processed data loaded.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_, X_train_transformed = load_processed_X_train('vocabulary_sample1', \n",
    "                                                          'X_train_processed_sample1',\n",
    "                                                           X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.938, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.958, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.979, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a logistic regression classifier\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "cv_score = cross_val_score(log_clf, X_train_transformed, y_train, cv=5, verbose=3)\n",
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test set\n",
    "test_vocab, X_test_transformed = preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the vocabulary is the same\n",
    "for ((w1, ct1), (w2, ct2)) in zip(test_vocab.items(), vocabulary_.items()):\n",
    "    try:\n",
    "        assert w1 == w2 and ct1 == ct2\n",
    "    except AssertionError:\n",
    "        print((w1, ct1), (w2, ct2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 85.71%\n",
      "Recall: 75.00%\n"
     ]
    }
   ],
   "source": [
    "# predict and calculate precision and recall\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Et voil√†‚ÄØ!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
