{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying Mr. Geron's Spam Classifier Notebook \n",
    "\n",
    "Code often borrowed from [Aur√©lien Geron's famous Jupyter Notebook on Classification.](https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb)\n",
    "\n",
    "Data can be pulled from [Apache SpamAssassin's old corpus.](http://spamassassin.apache.org/old/publiccorpus/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-07-22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import custom_functions as F # see custom module for code\n",
    "\n",
    "start_time = time.time()\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "dt_object = str(dt_object).split('.')[0]\n",
    "Date, StartTime = dt_object.split(' ')\n",
    "print('Revised on: ' + Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose \n",
    "\n",
    "Save preprocessed training and test data first - compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully downloaded.\n"
     ]
    }
   ],
   "source": [
    "F.get_data_if_needed('spam', 'easy_ham', '20030228')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2500 ham emails and 500 spam emails.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "spam_dir = os.path.join(data_dir, 'spam')\n",
    "ham_dir = os.path.join(data_dir, 'easy_ham')\n",
    "\n",
    "ham_filenames = [name for name in sorted(os.listdir(ham_dir)) if name != 'cmds']\n",
    "spam_filenames = [name for name in sorted(os.listdir(spam_dir)) if name != 'cmds']\n",
    "\n",
    "print('There are ' +str(len(ham_filenames)) + ' ham emails and ' + str(len(spam_filenames)) + ' spam emails.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting emails\n",
    "spam = F.extract_emails(_path=spam_dir, _names=spam_filenames)\n",
    "ham = F.extract_emails(_path=ham_dir, _names=ham_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test datasets\n",
    "\n",
    "We need to split the traing and test sets before gaining too much information on the test set and biasing ourselves in creating the features for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham + spam)\n",
    "y = np.array([0] * len(ham) + [1] * len(spam))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Train and Test sets - save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mr. Geron's pipeline - using stopwords\n",
    "preprocess_pipeline_original = Pipeline([\n",
    "    (\"email_to_wordcount\", F.EmailToWordCounterTransformer_revised(remove_stopwords=False)),\n",
    "    (\"wordcount_to_vector\", F.WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "# New pipeline without stopwords\n",
    "preprocess_pipeline_nostopwords= Pipeline([\n",
    "    (\"email_to_wordcount\", F.EmailToWordCounterTransformer_revised(remove_stopwords=True)),\n",
    "    (\"wordcount_to_vector\", F.WordCounterToVectorTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup directory \n",
    "path = 'processed_data'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names and paths\n",
    "\n",
    "# vocabulary\n",
    "vocab_path_original = os.path.join(path, ''.join(['vocabulary_original', '.json']))\n",
    "vocab_path_nostopwords = os.path.join(path, ''.join(['vocabulary_nostopwords', '.json']))\n",
    "\n",
    "# training data\n",
    "X_train_path_original = os.path.join(path, ''.join(['X_train_processed_original', '.npz']))\n",
    "X_train_path_nostopwords = os.path.join(path, ''.join(['X_train_processed_nostopwrods', '.npz']))\n",
    "\n",
    "# test data\n",
    "X_test_path_original = os.path.join(path, ''.join(['X_test_processed_original', '.npz']))\n",
    "X_test_path_nostopwords = os.path.join(path, ''.join(['X_test_processed_nostopwrods', '.npz']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary(vocabulary_path):\n",
    "    try:\n",
    "        with open(vocabulary_path, 'r') as fp:\n",
    "            vocabulary_ = json.load(fp)\n",
    "        return(vocabulary_)\n",
    "    except FileNotFoundError as e:  \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_matrix(filepath):\n",
    "    try:\n",
    "        X = np.load(filepath)\n",
    "        npz = scipy.sparse.coo_matrix((X['data'], (X['row'], X['col'])), shape=X['shape'])\n",
    "        return(npz)\n",
    "    except FileNotFoundError as e:  \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original \n",
    "X_train_transformed_original = load_sparse_matrix(X_train_path_original)\n",
    "X_test_transformed_original = load_sparse_matrix(X_test_path_original)\n",
    "\n",
    "# load nostopwords\n",
    "X_train_transformed_nostopwords = load_sparse_matrix(X_train_path_nostopwords)\n",
    "X_test_transformed_nostopwords = load_sparse_matrix(X_test_path_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess training and test data (6.5 mins)\n",
    "#\n",
    "## original\n",
    "#X_train_transformed_original = preprocess_pipeline_original.fit_transform(X_train)\n",
    "#X_test_transformed_original = preprocess_pipeline_original.fit_transform(X_test)\n",
    "#\n",
    "## no stopwords\n",
    "#X_train_transformed_nostopwords = preprocess_pipeline_nostopwords.fit_transform(X_train)\n",
    "#X_test_transformed_nostopwords = preprocess_pipeline_nostopwords.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocabulary\n",
    "def save_vocabulary(vocabulary_path, vocabulary_):\n",
    "    with open(vocabulary_path, 'w') as fp:\n",
    "        json.dump(vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sparse_matrix(filepath, X):\n",
    "    X_coo = X.tocoo()\n",
    "    row = X_coo.row\n",
    "    col = X_coo.col\n",
    "    data = X_coo.data\n",
    "    shape = X_coo.shape\n",
    "    np.savez(filepath, row=row, col=col, data=data, shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sparse_matrix(X_train_path_original, X_train_transformed_original)\n",
    "save_sparse_matrix(X_train_path_nostopwords, X_train_transformed_nostopwords)\n",
    "save_sparse_matrix(X_test_path_original, X_test_transformed_original)\n",
    "save_sparse_matrix(X_test_path_nostopwords, X_test_transformed_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test_processed_nostopwrods.npz',\n",
       " 'X_test_processed_original.npz',\n",
       " 'X_train_processed_nostopwrods.npz',\n",
       " 'X_train_processed_original.npz',\n",
       " 'X_train_processed_sample1.npz']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparse matrices in processed_data directory\n",
    "[x for x in os.listdir(path) if x.split('.')[1] == 'npz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to get the vocabulary_ ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
