{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying Mr. Geron's Spam Classifier Notebook  - Part II\n",
    "\n",
    "Code often borrowed from [Aur√©lien Geron's famous Jupyter Notebook on Classification.](https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb)\n",
    "\n",
    "Data can be pulled from [Apache SpamAssassin's old corpus.](http://spamassassin.apache.org/old/publiccorpus/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-07-19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import custom_functions as F # see custom module for code\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "dt_object = str(dt_object).split('.')[0]\n",
    "Date, StartTime = dt_object.split(' ')\n",
    "print('Revised on: ' + Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "TEST the previous notebook to ensure it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "Random sampling 10% of the data for quick troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully downloaded.\n"
     ]
    }
   ],
   "source": [
    "F.get_data_if_needed('spam', 'easy_ham', '20030228')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250.0, 50.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting emails\n",
    "data_dir = 'data'\n",
    "spam_dir = os.path.join(data_dir, 'spam')\n",
    "ham_dir = os.path.join(data_dir, 'easy_ham')\n",
    "\n",
    "ham_filenames = [name for name in sorted(os.listdir(ham_dir)) if name != 'cmds']\n",
    "spam_filenames = [name for name in sorted(os.listdir(spam_dir)) if name != 'cmds']\n",
    "\n",
    "len(ham_filenames)/10, len(spam_filenames)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "            \n",
    "ham_sample = random.sample(ham_filenames, 250)\n",
    "spam_sample = random.sample(spam_filenames, 50)\n",
    "\n",
    "spam = F.extract_emails(_path=spam_dir, _names=spam_sample)\n",
    "ham = F.extract_emails(_path=ham_dir, _names=ham_sample)\n",
    "\n",
    "len(ham), len(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test datasets\n",
    "\n",
    "We need to split the traing and test sets before gaining too much information on the test set and biasing ourselves in creating the features for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham + spam)\n",
    "y = np.array([0] * len(ham) + [1] * len(spam))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess, Train, Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stopwords included\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", F.EmailToWordCounterTransformer_revised(remove_stopwords=False)),\n",
    "    (\"wordcount_to_vector\", F.WordCounterToVectorTransformer_plusvocab()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIX\n",
    "\n",
    "How to fix the vocabulary problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary.\n",
      "Loading sparse matrix.\n",
      "Processed data loaded.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_, X_train_transformed = F.load_processed_X_train('vocabulary_sample1', \n",
    "                                                          'X_train_processed_sample1',\n",
    "                                                           preprocess_pipeline,\n",
    "                                                           X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.938, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.958, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.979, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a logistic regression classifier\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "cv_score = cross_val_score(log_clf, X_train_transformed, y_train, cv=5, verbose=3)\n",
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'WordCounterToVectorTransformer_plusvocab' object has no attribute 'vocabulary_'\n"
     ]
    }
   ],
   "source": [
    "# preprocess test set\n",
    "try:\n",
    "    test_vocab, X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the vocabulary is the same\n",
    "for ((w1, ct1), (w2, ct2)) in zip(test_vocab.items(), vocabulary_.items()):\n",
    "    try:\n",
    "        assert w1 == w2 and ct1 == ct2\n",
    "    except AssertionError:\n",
    "        print((w1, ct1), (w2, ct2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 85.71%\n",
      "Recall: 75.00%\n"
     ]
    }
   ],
   "source": [
    "# predict and calculate precision and recall\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
