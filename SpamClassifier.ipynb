{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying Mr. Geron's Spam Classifier Notebook \n",
    "\n",
    "Code often borrowed from [Aurélien Geron's famous Jupyter Notebook on Classification.](https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb)\n",
    "\n",
    "Data can be pulled from [Apache SpamAssassin's old corpus.](http://spamassassin.apache.org/old/publiccorpus/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-07-18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import custom_functions as F # see custom module for code\n",
    "\n",
    "start_time = time.time()\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "dt_object = str(dt_object).split('.')[0]\n",
    "Date, StartTime = dt_object.split(' ')\n",
    "print('Revised on: ' + Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully downloaded.\n"
     ]
    }
   ],
   "source": [
    "F.get_data_if_needed('spam', 'easy_ham', '20030228')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2500 ham emails and 500 spam emails.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "spam_dir = os.path.join(data_dir, 'spam')\n",
    "ham_dir = os.path.join(data_dir, 'easy_ham')\n",
    "\n",
    "ham_filenames = [name for name in sorted(os.listdir(ham_dir)) if name != 'cmds']\n",
    "spam_filenames = [name for name in sorted(os.listdir(spam_dir)) if name != 'cmds']\n",
    "\n",
    "print('There are ' +str(len(ham_filenames)) + ' ham emails and ' + str(len(spam_filenames)) + ' spam emails.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting emails\n",
    "spam = F.extract_emails(_path=spam_dir, _names=spam_filenames)\n",
    "ham = F.extract_emails(_path=ham_dir, _names=ham_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email headers can be lengthy and contain more than 50% of the information, however, a lot of this information is not standard across headers and also might not be informative for ML on a first pass, except for the Subject and the Content-Type perhaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path               : <martin@srv0.ems.ed.ac.uk>\n",
      "Delivered-To              : zzzz@localhost.netnoteinc.com\n",
      "Received                  : from localhost (localhost [127.0.0.1])\tby phobos.l\n",
      "Received                  : from phobos [127.0.0.1]\tby localhost with IMAP (fe\n",
      "Received                  : from n11.grp.scd.yahoo.com (n11.grp.scd.yahoo.com \n",
      "X-Egroups-Return          : sentto-2242572-52738-1030024499-zzzz=spamassassin.\n",
      "Received                  : from [66.218.66.94] by n11.grp.scd.yahoo.com with \n",
      "X-Sender                  : martin@srv0.ems.ed.ac.uk\n",
      "X-Apparently-To           : zzzzteana@yahoogroups.com\n",
      "Received                  : (EGP: mail-8_1_0_1); 22 Aug 2002 13:54:59 -0000\n",
      "Received                  : (qmail 43039 invoked from network); 22 Aug 2002 13\n",
      "Received                  : from unknown (66.218.66.216) by m1.grp.scd.yahoo.c\n",
      "Received                  : from unknown (HELO haymarket.ed.ac.uk) (129.215.12\n",
      "Received                  : from srv0.ems.ed.ac.uk (srv0.ems.ed.ac.uk [129.215\n",
      "Received                  : from EMS-SRV0/SpoolDir by srv0.ems.ed.ac.uk (Mercu\n",
      "Received                  : from SpoolDir by EMS-SRV0 (Mercury 1.44); 22 Aug 0\n",
      "Organization              : Management School\n",
      "To                        : zzzzteana@yahoogroups.com\n",
      "Message-Id                : <3D64FB27.18538.63DEC17@localhost>\n",
      "Priority                  : normal\n",
      "X-Mailer                  : Pegasus Mail for Windows (v4.01)\n",
      "Content-Description       : Mail message body\n",
      "From                      : Martin Adamson <martin@srv0.ems.ed.ac.uk>\n",
      "MIME-Version              : 1.0\n",
      "Mailing-List              : list zzzzteana@yahoogroups.com; contact    fortean\n",
      "Delivered-To              : mailing list zzzzteana@yahoogroups.com\n",
      "Precedence                : bulk\n",
      "List-Unsubscribe          : <mailto:zzzzteana-unsubscribe@yahoogroups.com>\n",
      "Date                      : Thu, 22 Aug 2002 14:54:25 +0100\n",
      "Subject                   : [zzzzteana] Playboy wants to go out with a bang\n",
      "Reply-To                  : zzzzteana@yahoogroups.com\n",
      "Content-Type              : text/plain; charset=\"ISO-8859-1\"\n",
      "Content-Transfer-Encoding : 8bit\n",
      "X-MIME-Autoconverted      : from quoted-printable to 8bit by dogma.slashnull.o\n"
     ]
    }
   ],
   "source": [
    "# example ham header\n",
    "for header, value in ham[6].items():\n",
    "    padding=25-len(header)\n",
    "    print(header + ' '*padding, ':', value[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path               : <jamesalabi@mail.com>\n",
      "Delivered-To              : zzzz@localhost.spamassassin.taint.org\n",
      "Received                  : from localhost (localhost [127.0.0.1])\tby phobos.l\n",
      "Received                  : from phobos [127.0.0.1]\tby localhost with IMAP (fe\n",
      "Received                  : from webnote.net (mail.webnote.net [193.120.211.21\n",
      "Received                  : from ok61094.com ([217.78.76.138]) by webnote.net \n",
      "Message-Id                : <200208241717.SAA16606@webnote.net>\n",
      "From                      : \"Dr.James Ologun\" <jamesalabi@mail.com>\n",
      "Reply-To                  : jamesalabi@mail.com\n",
      "To                        : zzzz-sa-listinfo@spamassassin.taint.org\n",
      "Date                      : Sat, 24 Aug 2002 20:18:02 -0700\n",
      "Subject                   : Immediate Reply Needed\n",
      "X-Mailer                  : Microsoft Outlook Express 5.00.2919.6900 DM\n",
      "MIME-Version              : 1.0\n",
      "Content-Type              : text/plain; charset=\"us-ascii\"\n",
      "X-MIME-Autoconverted      : from quoted-printable to 8bit by dogma.slashnull.o\n",
      "Content-Transfer-Encoding : 8bit\n"
     ]
    }
   ],
   "source": [
    "# example spam header\n",
    "for header, value in spam[83].items():\n",
    "    padding=25-len(header)\n",
    "    print(header + ' '*padding, ':', value[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Scotsman - 22 August 2002\n",
      "\n",
      " Playboy wants to go out with a bang \n",
      " \n",
      " \n",
      " AN AGEING Berlin playboy has come up with an unusual offer to lure women into\n",
      " his bed - by promising the last woman he sleeps with an inheritance of 250,000\n",
      " (£160,000). \n",
      " \n",
      " Rolf Eden, 72, a Berlin disco owner famous for his countless sex partners,\n",
      " said he could imagine no better way to die than in the arms of an attractive\n",
      " young woman - preferably under 30. \n",
      " \n",
      " \"I put it all in my last will and testament - the last woman who sleeps with\n",
      " me gets all the money,\" Mr Eden told Bild newspaper. \n",
      " \n",
      " \"I want to pass away in the most beautiful moment of my life. First a lot of\n",
      " fun with a beautiful woman, then wild sex, a final orgasm - and it will all\n",
      " end with a heart attack and then Im gone.\" \n",
      " \n",
      " Mr Eden, who is selling his nightclub this year, said applications should be\n",
      " sent in quickly because of his age. \"It could end very soon,\" he said.\n",
      "\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "# example ham body\n",
    "print(ham[6].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Sir,\n",
      "\n",
      "I am Dr James Alabi, the chairman of contract\n",
      "award and review committee set up by the federal\n",
      "government of Nigeria under the new civilian\n",
      "dispensation to award new contracts and review\n",
      "existing ones.\n",
      "I came to know of you in my search for a reliable and\n",
      "reputable person to handle a very confidential\n",
      "transaction, which involves the transfer of a huge\n",
      "sum of money to a foreign account. \n",
      "\n",
      "There were series of contracts executed by a \n",
      "consortium of multi-nationals in the oil industry in\n",
      "favor of N.N.P.C. The original values of these \n",
      "contracts were deliberately over invoiced to the sum\n",
      "of US$12,320,000.00 (Twelve Million Three Hundred and Twenty Thousand \n",
      "United\n",
      "State Dollars). This amount has now been approved and\n",
      "is now ready to be transferred being that the\n",
      "companies\n",
      "that actually executed these contracts have been\n",
      "fully Paid and the projects officially commissioned. \n",
      "\n",
      "\n",
      "Consequently, my colleagues and I are willing to \n",
      "transfer the total amount to your account for\n",
      "subsequent disbursement, since we as civil servants\n",
      "are prohibited by the code of conduct bureau (civil\n",
      "service law) from operating and/or opening foreign \n",
      "accounts in our names. Needless to say, the trust\n",
      "reposed on you at this juncture is enormous, in \n",
      "return, we have agreed to offer you 20% of the \n",
      "transferred sum, while 10% shall be set aside for\n",
      "incidental expenses (internal and external) between\n",
      "both parties in the course of the transaction you will\n",
      "be mandated to remit the balance to other accounts\n",
      "in due course. \n",
      "\n",
      "Modalities have been worked out at the highest level\n",
      "of the Ministry of Finance and the Central Bank of \n",
      "Nigeria for the immediate transfer of the funds\n",
      "within 7 working days subject to your satisfaction of\n",
      "the above stated terms. \n",
      "Our assurance is that your role is risk free. To \n",
      "\n",
      "accord this transaction the legality it deserves and\n",
      "for mutual security of the funds the whole approval\n",
      "procedures will officially and legally processed\n",
      "with your name or the name of any company you may\n",
      "nominate as the bonefide beneficiary. \n",
      "Once more I want you to understand that having put\n",
      "in over twenty-five years in the civil service of my\n",
      "country, I am averse to having my image and career \n",
      "dented. This matter should therefore be treated with\n",
      "utmost secrecy and urgency it deserves. \n",
      "\n",
      "Please you should signify your intention to assist\n",
      "by sending me a reply to this email to state your position on this\n",
      "transaction, if favorable we will take further steps\n",
      "to brief you the full details of this viable \n",
      "transaction. \n",
      "I want to assure you that this business proposal is\n",
      "100% risk free as we have done our homework\n",
      "properly. \n",
      "\n",
      "I quite believe that you will protect our interest\n",
      "by taking this deal strictly confidential, as we are\n",
      "still in government service, which we intend to\n",
      "retire from in full honor.\n",
      "\n",
      "Kindly expedite action as we are behind schedule to\n",
      "enable us include this transfer in the next batch\n",
      "which would constitute the new quarter payments\n",
      "for the 2002 financial year. \n",
      "\n",
      "Thank you and God bless.\n",
      "\n",
      "Dr James Alabi\n"
     ]
    }
   ],
   "source": [
    "# example spam body\n",
    "print(spam[83].get_content().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Email Structures can be complex.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<email.message.EmailMessage at 0x246060bcc88>,\n",
       " <email.message.EmailMessage at 0x246060a4f60>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# payload can return single email or a list of objects\n",
    "ham[13].get_payload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/plain\n",
      "application/pgp-signature\n"
     ]
    }
   ],
   "source": [
    "# an email.message can be text/plain, text/html, and various other categories\n",
    "for email in ham[13].get_payload():\n",
    "    print(email.get_content_type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2408: text/plain\n",
      "  66: multipart(text/plain | application/pgp-signature)\n",
      "   8: multipart(text/plain | text/html)\n",
      "   4: multipart(text/plain | text/plain)\n",
      "   3: multipart(text/plain)\n",
      "   2: multipart(text/plain | application/octet-stream)\n",
      "   1: multipart(text/plain | text/enriched)\n",
      "   1: multipart(text/plain | application/ms-tnef | text/plain)\n",
      "   1: multipart(multipart(text/plain | text/plain | text/plain) | application/pgp-signature)\n",
      "   1: multipart(text/plain | video/mng)\n",
      "   1: multipart(text/plain | multipart(text/plain))\n",
      "   1: multipart(text/plain | application/x-pkcs7-signature)\n",
      "   1: multipart(text/plain | multipart(text/plain | text/plain) | text/rfc822-headers)\n",
      "   1: multipart(text/plain | multipart(text/plain | text/plain) | multipart(multipart(text/plain | application/x-pkcs7-signature)))\n",
      "   1: multipart(text/plain | application/x-java-applet)\n"
     ]
    }
   ],
   "source": [
    "# using Mr.Geron's nifty structure counters (see custom code)\n",
    "\n",
    "# most common ham structures\n",
    "for i in F.structures_counter(ham).most_common():\n",
    "    padding = 4-len(str(i[1]))\n",
    "    print(' '*padding + str(i[1]) + ': ' +i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 218: text/plain\n",
      " 183: text/html\n",
      "  45: multipart(text/plain | text/html)\n",
      "  20: multipart(text/html)\n",
      "  19: multipart(text/plain)\n",
      "   5: multipart(multipart(text/html))\n",
      "   3: multipart(text/plain | image/jpeg)\n",
      "   2: multipart(text/html | application/octet-stream)\n",
      "   1: multipart(text/plain | application/octet-stream)\n",
      "   1: multipart(text/html | text/plain)\n",
      "   1: multipart(multipart(text/html) | application/octet-stream | image/jpeg)\n",
      "   1: multipart(multipart(text/plain | text/html) | image/gif)\n",
      "   1: multipart/alternative\n"
     ]
    }
   ],
   "source": [
    "# most common spam structures\n",
    "for i in F.structures_counter(spam).most_common():\n",
    "    padding = 4-len(str(i[1]))\n",
    "    print(' '*padding + str(i[1]) + ': ' +i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test datasets\n",
    "\n",
    "We need to split the traing and test sets before gaining too much information on the test set and biasing ourselves in creating the features for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham + spam)\n",
    "y = np.array([0] * len(ham) + [1] * len(spam))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\n",
      "<head>\n",
      "<title>Home Page</title>\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "\n",
      "<p align=\"center\"><font color=\"#000000\" face=\"Arial\" size=\"+0\"><b><IMG SRC=\"http://mail4.mortgages101.net/logo.php?id=88&id2=1143953\"></p>\n",
      "\n",
      "<p align=\"center\">If this promotion has reached you in error and you would prefer not to\n",
      "receive marketing messages from us, please send an email to&nbsp; <a\n",
      "href=\"mailto:cease-and-desist@mortgages101.net\">cease-and-desist@mortgages101.net</a>\n",
      "&nbsp; (all one word, no spaces) giving us the email address in question or call\n",
      "1-888-748-7751 for further assistance.</p>\n",
      "\n",
      "<p align=\"center\"><u>Gain access to a</b></font><font size=\"+1\" color=\"#000000\"\n",
      "face=\"Arial\"> <i><b>Vast Network Of Qualified Lenders at Nationwide Network!</b></i></font></u></p>\n",
      "\n",
      "<p align=\"center\"><font color=\"#000000\" face=\"Arial\">This is a zero-cost service which\n",
      "enables you to shop for a mortgage conveniently from your home computer. &nbsp; Our\n",
      "nationwide database will give you access to lenders with a variety of loan programs that\n",
      "will work for Excellent, Good, Fair or even Poor Credit! </font><br>\n",
      "&nbsp; <font face=\"Arial,Helvetica\"><b><font color=\"#000000\">We will choose up to 3 mortgage companies\n",
      "from our database of&nbsp; registered brokers/lenders.</font></b> <b><font\n",
      "face=\"Arial,Helvetica\">Each will contact you to offer you their best rate and terms - at\n",
      "no charge.</font></b><br>\n",
      "&nbsp; <br>\n",
      "&nbsp;</font><b><fo ...\n"
     ]
    }
   ],
   "source": [
    "HTML_spam = [email for email in X_train[y_train==1] if email.get_content_type() == \"text/html\"]\n",
    "print(HTML_spam[9].get_content().strip()[:1400], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If this promotion has reached you in error and you would prefer not to\n",
      "receive marketing messages from us, please send an email to   HYPERLINK cease-and-desist@mortgages101.net\n",
      "  (all one word, no spaces) giving us the email address in question or call\n",
      "1-888-748-7751 for further assistance.\n",
      "Gain access to a Vast Network Of Qualified Lenders at Nationwide Network!\n",
      "This is a zero-cost service which\n",
      "enables you to shop for a mortgage conveniently from your home computer.   Our\n",
      "nationwide database will give you access to lenders with a variety of loan programs that\n",
      "will work for Excellent, Good, Fair or even Poor Credit!\n",
      "  We will choose up to 3 mortgage companies\n",
      "from our database of  registered brokers/lenders. Each will contact you to offer you their best rate and terms - at\n",
      "no charge.\n",
      " \n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "print(F.html_to_plaintext(HTML_spam[9].get_content())[:800], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Here I start a comparison of Stemmers and Lemmatization. \n",
    "\n",
    "Thought: \n",
    "\n",
    "* test each change, one at a time, always comparing with the baseline which is Mr. Geron's original models:\n",
    "1. Rerun with the Lancaster Stemmer\n",
    "2. Rerun with Lemmatization \n",
    "3. Rerun removing Stop Words\n",
    "\n",
    "Have we taken into consideration the number of words vs number of tokens (pct unique)? \n",
    "\n",
    "How about the size of the vocabulary?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter:    Barbaric    => barbar\n",
      "Lancaster: Barbaric    => barb\n",
      "Porter:    Barbarian   => barbarian\n",
      "Lancaster: Barbarian   => barb\n",
      "Porter:    Confusion   => confus\n",
      "Lancaster: Confusion   => confus\n",
      "Porter:    Confusing   => confus\n",
      "Lancaster: Confusing   => confus\n",
      "Porter:    Confer      => confer\n",
      "Lancaster: Confer      => conf\n",
      "Porter:    Conferred   => confer\n",
      "Lancaster: Conferred   => confer\n",
      "Porter:    Confabulate => confabul\n",
      "Lancaster: Confabulate => confab\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer() # more aggressive stemmer --> better?\n",
    "wordlen = 0\n",
    "for word in (\"Barbaric\", \"Barbarian\", \"Confusion\", \"Confusing\", \"Confer\", \"Conferred\", \"Confabulate\"):       \n",
    "    padding = 11-len(word)\n",
    "    print(\"Porter:    \" +word +padding*' ' + \" => \" +porter.stem(word))\n",
    "    print(\"Lancaster: \" +word +padding*' ' + \" => \" +lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "-----               -----               \n",
      "Barbaric            Barbaric            \n",
      "barbarians          barbarian           \n",
      "confused            confused            \n",
      "confusion           confusion           \n",
      "conferred           conferred           \n",
      "confabulations      confabulation       \n",
      "confabulate         confabulate         \n",
      "Biblical            Biblical            \n",
      "meanderings         meanderings         \n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"Barbaric barbarians! confused, confusion conferred - confabulations confabulate Biblical meanderings.\"\n",
    "punctuations=\"?:!.,;-\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "print(\"{0:20}{1:20}\".format(\"-----\",\"-----\"))\n",
    "\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,lemma.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email to Word Counter\n",
    "\n",
    "Mr. Geron:\n",
    "\n",
    "\"*We are ready to put all this together into a transformer that we will use to convert emails to word counters. Note that we split sentences into words using Python's `split()` method, which uses whitespaces for word boundaries. This works for many written languages, but not all. For example, Chinese and Japanese scripts generally don't use spaces between words, and Vietnamese often uses spaces even between syllables. It's okay in this exercise, because the dataset is (mostly) in English.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Fri, 6 Sep 2002, Russell Turpin wrote:\n",
      "\n",
      "> Don't swallow too quickly what you have read about\n",
      "> more traditional cultures, today or in the past. Do\n",
      "\n",
      "I don't swallow ;>\n",
      "\n",
      "I was just offering anecdotal first-hand experiences from a number of\n",
      "cultures indicating 1) we apparently have a problem 2) which requires more\n",
      "than ad hoc hand-waving approach (it's trivial! it's obvious! all we have\n",
      "to do is XY!).\n",
      "\n",
      "> we have any statistics on the poor man's divorce from\n",
      "> centuries past? Are you so sure that the kids in 18th\n",
      "\n",
      "That's easy. Divorce didn't happen. The church and the society looked\n",
      "after that. Only relatively recently that privilege was granted to kings, \n",
      "and only very recently to commoners.\n",
      "\n",
      "> century England were any more \"functional\" than those\n",
      "> today? What about 20th century Saudi Arabia?\n",
      "\n",
      "Is Saudi Arabia a meaningful emigration source?\n",
      " \n",
      "> >At least from the viewpoint of demographics sustainability and \n",
      "> >counterpressure to gerontocracy and resulting innovatiophobia we're doing\n",
      "> >something wrong.\n",
      "> \n",
      "> Granting your first two points, I'm skeptical about\n",
      "> the last. Do you see ANY signs that America specifically\n",
      "\n",
      "I wasn't talking about the US specifically. (Though the demographics \n",
      "problem exists there as well, albeit not in that extent we Eurotrash are \n",
      "facing right now).\n",
      "\n",
      "> or the west generally are suffering from lack of\n",
      "> innovation, vis-a-vis youth nations such as Iran? The\n",
      "\n",
      "1) I'm seeing lack of innovation, and -- more disturbing -- trend towards\n",
      "even less innovation by an autocatalytic process (gerontocracy favors\n",
      "gerontocracy).\n",
      "\n",
      "> last I read, the third generation of the revolution all\n",
      "> (a) want to move to America, and (b) failing that, are\n",
      "> importing everything they can American.\n",
      "\n",
      "My point was that the west, US first and foremost, importing innovation\n",
      "carriers and working against bad trend in the demographics by large scale\n",
      "import. While this kinda, sorta works on the short run, this is not\n",
      "something sustainable.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exampleX = X_train[11]\n",
    "\n",
    "print(exampleX.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Mr. Geron's counter does not remove stop words - I'm curious to test whether this is a good move or whether stop words prevent classifiers from further gleaning information from more useful words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'the': 16, 'that': 8, 'and': 8, 'number': 6, 'i': 6, 'to': 6, 'a': 5, 'of': 5, 'we': 5, 't': 4, 'have': 4, 'about': 4, 'more': 4, 'in': 4, 'do': 4, 'from': 4, 's': 4, 'are': 4, 'innov': 4, 'on': 3, 'you': 3, 'wa': 3, 'first': 3, 'is': 3, 'ani': 3, 'centuri': 3, 'demograph': 3, 'gerontocraci': 3, 'import': 3, 'don': 2, 'swallow': 2, 'what': 2, 'read': 2, 'cultur': 2, 'today': 2, 'or': 2, 'past': 2, 'hand': 2, 'problem': 2, 'than': 2, 'it': 2, 'all': 2, 'divorc': 2, 'numberth': 2, 'onli': 2, 'recent': 2, 'grant': 2, 'saudi': 2, 'arabia': 2, 'sustain': 2, 'someth': 2, 'point': 2, 'm': 2, 'last': 2, 'see': 2, 'america': 2, 'specif': 2, 'us': 2, 'as': 2, 'not': 2, 'west': 2, 'gener': 2, 'lack': 2, 'vi': 2, 'trend': 2, 'by': 2, 'work': 2, 'thi': 2, 'fri': 1, 'sep': 1, 'russel': 1, 'turpin': 1, 'wrote': 1, 'too': 1, 'quickli': 1, 'tradit': 1, 'just': 1, 'offer': 1, 'anecdot': 1, 'experi': 1, 'indic': 1, 'appar': 1, 'which': 1, 'requir': 1, 'ad': 1, 'hoc': 1, 'wave': 1, 'approach': 1, 'trivial': 1, 'obviou': 1, 'xy': 1, 'statist': 1, 'poor': 1, 'man': 1, 'so': 1, 'sure': 1, 'kid': 1, 'easi': 1, 'didn': 1, 'happen': 1, 'church': 1, 'societi': 1, 'look': 1, 'after': 1, 'rel': 1, 'privileg': 1, 'king': 1, 'veri': 1, 'common': 1, 'england': 1, 'were': 1, 'function': 1, 'those': 1, 'meaning': 1, 'emigr': 1, 'sourc': 1, 'at': 1, 'least': 1, 'viewpoint': 1, 'counterpressur': 1, 'result': 1, 'innovatiophobia': 1, 're': 1, 'wrong': 1, 'your': 1, 'two': 1, 'skeptic': 1, 'sign': 1, 'wasn': 1, 'talk': 1, 'though': 1, 'exist': 1, 'there': 1, 'well': 1, 'albeit': 1, 'extent': 1, 'eurotrash': 1, 'face': 1, 'right': 1, 'now': 1, 'suffer': 1, 'youth': 1, 'nation': 1, 'such': 1, 'iran': 1, 'disturb': 1, 'toward': 1, 'even': 1, 'less': 1, 'an': 1, 'autocatalyt': 1, 'process': 1, 'favor': 1, 'third': 1, 'revolut': 1, 'want': 1, 'move': 1, 'b': 1, 'fail': 1, 'everyth': 1, 'they': 1, 'can': 1, 'american': 1, 'my': 1, 'foremost': 1, 'carrier': 1, 'against': 1, 'bad': 1, 'larg': 1, 'scale': 1, 'while': 1, 'kinda': 1, 'sorta': 1, 'short': 1, 'run': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeronsCounter = F.EmailToWordCounterTransformer_revised(remove_stopwords=False)\n",
    "GeronsCounter.fit_transform([exampleX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'number': 6, 'innov': 4, 'first': 3, 'centuri': 3, 'demograph': 3, 'gerontocraci': 3, 'import': 3, 'swallow': 2, 'read': 2, 'cultur': 2, 'today': 2, 'past': 2, 'hand': 2, 'problem': 2, 'divorc': 2, 'numberth': 2, 'recent': 2, 'grant': 2, 'saudi': 2, 'arabia': 2, 'sustain': 2, 'someth': 2, 'point': 2, 'last': 2, 'see': 2, 'america': 2, 'specif': 2, 'us': 2, 'west': 2, 'gener': 2, 'lack': 2, 'vi': 2, 'trend': 2, 'work': 2, 'fri': 1, 'sep': 1, 'russel': 1, 'turpin': 1, 'wrote': 1, 'quickli': 1, 'tradit': 1, 'offer': 1, 'anecdot': 1, 'experi': 1, 'indic': 1, 'appar': 1, 'requir': 1, 'ad': 1, 'hoc': 1, 'wave': 1, 'approach': 1, 'trivial': 1, 'obviou': 1, 'xy': 1, 'statist': 1, 'poor': 1, 'man': 1, 'sure': 1, 'kid': 1, 'easi': 1, 'happen': 1, 'church': 1, 'societi': 1, 'look': 1, 'rel': 1, 'privileg': 1, 'king': 1, 'common': 1, 'england': 1, 'function': 1, 'meaning': 1, 'emigr': 1, 'sourc': 1, 'least': 1, 'viewpoint': 1, 'counterpressur': 1, 'result': 1, 'innovatiophobia': 1, 'wrong': 1, 'two': 1, 'skeptic': 1, 'sign': 1, 'talk': 1, 'though': 1, 'exist': 1, 'well': 1, 'albeit': 1, 'extent': 1, 'eurotrash': 1, 'face': 1, 'right': 1, 'suffer': 1, 'youth': 1, 'nation': 1, 'iran': 1, 'disturb': 1, 'toward': 1, 'even': 1, 'less': 1, 'autocatalyt': 1, 'process': 1, 'favor': 1, 'third': 1, 'revolut': 1, 'want': 1, 'move': 1, 'b': 1, 'fail': 1, 'everyth': 1, 'american': 1, 'foremost': 1, 'carrier': 1, 'bad': 1, 'larg': 1, 'scale': 1, 'kinda': 1, 'sorta': 1, 'short': 1, 'run': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewCounter = F.EmailToWordCounterTransformer_revised(remove_stopwords=True)\n",
    "NewCounter.fit_transform([exampleX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also be useful to test whether including unique words, even if they appear only once, is useful. Words like `innovatiophobia`, `autocatalyt`, and `counterpressur` might be unique because of their length, so we wouldn't need to consult a dictionary of most common words which might incur expensive processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counter to Vector Transformer\n",
    "\n",
    "Mr. Geron: \n",
    "\n",
    "\"*Now we have the word counts, and we need to convert them to vectors. For this, we will build another transformer whose `fit()` method will build the vocabulary (an ordered list of the most common words) and whose `transform()` method will use the vocabulary to convert word counts to vectors. The output is a sparse matrix.*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 175 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeronsWordCounts = GeronsCounter.fit_transform([exampleX])\n",
    "\n",
    "countertovec = F.WordCounterToVectorTransformer()\n",
    "sparsematrix1 = countertovec.fit_transform(GeronsWordCounts)\n",
    "sparsematrix1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a potential way to limit a list to interesting words from Mr.Geron's counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 the\n",
      "8 that\n",
      "8 and\n",
      "6 number\n",
      "6 i\n",
      "6 to\n",
      "5 a\n",
      "5 of\n",
      "5 we\n",
      "4 t\n",
      "4 have\n",
      "4 about\n",
      "4 more\n",
      "4 in\n",
      "4 do\n",
      "4 from\n",
      "4 s\n",
      "4 are\n",
      "4 innov\n",
      "3 on\n",
      "3 you\n",
      "3 wa\n",
      "3 first\n",
      "3 is\n",
      "3 ani\n",
      "3 centuri\n",
      "3 demograph\n",
      "3 gerontocraci\n",
      "3 import\n",
      "2 don\n",
      "2 swallow\n",
      "2 what\n",
      "2 read\n",
      "2 cultur\n",
      "2 today\n",
      "2 or\n",
      "2 past\n",
      "2 hand\n",
      "2 problem\n",
      "2 than\n",
      "2 it\n",
      "2 all\n",
      "2 divorc\n",
      "2 numberth\n",
      "2 onli\n",
      "2 recent\n",
      "2 grant\n",
      "2 saudi\n",
      "2 arabia\n",
      "2 sustain\n",
      "2 someth\n",
      "2 point\n",
      "2 m\n",
      "2 last\n",
      "2 see\n",
      "2 america\n",
      "2 specif\n",
      "2 us\n",
      "2 as\n",
      "2 not\n",
      "2 west\n",
      "2 gener\n",
      "2 lack\n",
      "2 vi\n",
      "2 trend\n",
      "2 by\n",
      "2 work\n",
      "2 thi\n",
      "1 approach\n",
      "1 privileg\n",
      "1 function\n",
      "1 viewpoint\n",
      "1 counterpressur\n",
      "1 innovatiophobia\n",
      "1 eurotrash\n",
      "1 autocatalyt\n",
      "1 american\n",
      "1 foremost\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "for i in range(len(countertovec.vocabulary_)):\n",
    "    \n",
    "    ct = sparsematrix1.toarray()[0][i+1]\n",
    "    \n",
    "    word, count = list(countertovec.vocabulary_.items())[i]\n",
    "    if ct > 1:\n",
    "        print(ct, word)\n",
    "    elif ct == 1 and len(word) > 7:\n",
    "        print(ct, word)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess, Train, Validate using stopwords (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mr. Geron's pipeline\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", F.EmailToWordCounterTransformer_revised(remove_stopwords=False)),\n",
    "    (\"wordcount_to_vector\", F.WordCounterToVectorTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickled_file(filename, _pipeline):\n",
    "    _path = 'pickled_files'\n",
    "    if not os.path.exists(_path):\n",
    "        os.mkdir(_path)\n",
    "        \n",
    "    filepath = os.path.join(_path, ''.join([filename, '.pickle']))\n",
    "               \n",
    "    try:\n",
    "        X_train_transformed = pickle.load(open(os.path.join(filepath, 'rb')))\n",
    "        print('Loading X_train_transformed..')\n",
    "                                          \n",
    "                                          \n",
    "    except FileNotFoundError as e:\n",
    "        \n",
    "        print(e)\n",
    "        X_train_transformed = _pipeline.fit_transform(X_train)\n",
    "        \n",
    "        # pickle the model for future ease\n",
    "        pickle.dump(X_train_transformed, open(filepath, 'wb'))\n",
    "        \n",
    "        return(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'pickled_files\\\\X_train_transformed_stopwordsFalse.pickle\\\\rb'\n"
     ]
    }
   ],
   "source": [
    "# preprocess data if need be\n",
    "X_train_transformed = load_pickled_file('X_train_transformed_stopwordsFalse', preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.981, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.990, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.985, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.990, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.990, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9870833333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a logistic regression classifier\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "cv_score = cross_val_score(log_clf, X_train_transformed, y_train, cv=5, verbose=3)\n",
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 96.88%\n",
      "Recall: 97.89%\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rinse & Repeat: without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'pickled_files\\\\X_train_transformed_stopwordsTrue.pickle\\\\rb'\n"
     ]
    }
   ],
   "source": [
    "# New pipeline without stopwords\n",
    "preprocess_pipeline_NEW = Pipeline([\n",
    "    (\"email_to_wordcount\", F.EmailToWordCounterTransformer_revised(remove_stopwords=True)),\n",
    "    (\"wordcount_to_vector\", F.WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = load_pickled_file('X_train_transformed_stopwordsTrue', preprocess_pipeline_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.985, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.988, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.983, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.977, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.988, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9841666666666666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "cv_score = cross_val_score(log_clf, X_train_transformed, y_train, cv=5, verbose=3)\n",
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 98.85%\n",
      "Recall: 90.53%\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed = preprocess_pipeline_NEW.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords increases precision while lowering recall in this one particular instance. The trade-off rate between precision and recall in the second classifier is perhaps justified - a user might prefer seeing a few spam emails in her inbox (lower recall) to having her ham be incorrectly sent to the spam folder (lower precision).\n",
    "\n",
    "[TODO: is there a logic behind lower recall and higher precision when removing stopwords? Does it generalize (more tests)?]\n",
    "\n",
    "[TODO: compare with lemmatized words]\n",
    "\n",
    "[TODO: compare with shorter list of most significant words]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 170.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "# with no pickling, it takes 412 seconds\n",
    "# with pickling, it takes 170 seconds\n",
    "\n",
    "secs = round(time.time() - start_time, 1)\n",
    "print(''.join(['Time elapsed: ', str(secs), ' seconds.']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
